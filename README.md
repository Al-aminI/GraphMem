# üß† GraphMem

## **The Human Brain for Your AI Agents**

[![PyPI](https://img.shields.io/pypi/v/agentic-graph-mem.svg)](https://pypi.org/project/agentic-graph-mem/)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub](https://img.shields.io/badge/github-Al--aminI/GraphMem-blue.svg)](https://github.com/Al-aminI/GraphMem)

> **"Memory is the treasury and guardian of all things."** ‚Äî Cicero

GraphMem is the **first memory system that thinks like a human brain**. It doesn't just store data‚Äîit **forgets**, **consolidates**, **prioritizes**, and **evolves** exactly like biological memory does.

**This is the future of enterprise AI agents.**

---

## üß¨ Why GraphMem Changes Everything

### The Problem with Current AI Memory

Every production AI agent faces the same crisis:

```
Day 1:     "Who is the CEO?" ‚Üí "Elon Musk" ‚úÖ
Day 100:   Context window: OVERFLOW üí•
Day 365:   "Who is the CEO?" ‚Üí "John... or was it Jane... maybe Elon?" ü§Ø
```

**Vector databases don't forget.** They accumulate garbage until your agent drowns in irrelevant, conflicting, outdated information.

### The GraphMem Solution: Memory That Thinks

GraphMem implements the **four pillars of human memory**:

| Human Brain | GraphMem | Why It Matters |
|-------------|----------|----------------|
| üß† **Forgetting Curve** | Memory Decay | Irrelevant memories fade naturally |
| üîó **Neural Networks** | Knowledge Graph | Relationships between concepts |
| ‚≠ê **Importance Weighting** | PageRank Centrality | Hub concepts (Elon Musk) > peripheral ones |
| ‚è∞ **Episodic Memory** | Temporal Validity | "CEO in 2015" vs "CEO now" |

---

## üöÄ Revolutionary Features

### 1. üï∞Ô∏è Point-in-Time Memory (Temporal Validity)

**"Who was CEO in 2015?"** ‚Äî No other memory system can answer this.

```python
from datetime import datetime
from graphmem import GraphMem, MemoryConfig

memory = GraphMem(config)

# GraphMem tracks WHEN facts are true
memory.ingest("John Smith was CEO of ACME from 2010 to 2018")
memory.ingest("Jane Doe became CEO of ACME in July 2018")

# Point-in-time queries - like human episodic memory!
memory.query("Who was CEO in 2015?")      # ‚Üí "John Smith" ‚úÖ
memory.query("Who is CEO now?")           # ‚Üí "Jane Doe" ‚úÖ
memory.query("Who was CEO in 2019?")      # ‚Üí "Jane Doe" ‚úÖ
```

**Use Cases:**
- üìã "What contracts were active last quarter?"
- üëî "Who was our legal counsel before 2020?"
- üìà "What was our strategy during COVID?"

### 2. ‚≠ê PageRank Centrality (Hub Detection)

GraphMem uses **Google's PageRank algorithm** to identify important entities:

```
Importance Formula: œÅ(e) = w1¬∑f1 + w2¬∑f2 + w3¬∑f3 + w4¬∑f4

where:
  f1 = Temporal recency    (recent = important)
  f2 = Access frequency    (used often = important)  
  f3 = PageRank centrality (well-connected = important) ‚Üê NEW!
  f4 = User feedback       (explicit signals)
```

**Result:** "Elon Musk" (connected to Tesla, SpaceX, Neuralink) scores **3x higher** than "Austin, Texas" (connected only to Tesla HQ).

```python
# PageRank automatically identifies hub entities
Elon Musk:      PR = 1.000 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  # Hub - many connections
Tesla:          PR = 0.774 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       # Important company
Austin:         PR = 0.520 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            # Just a location
```

### 3. üß† Self-Evolution (Like Human Memory)

```python
memory.evolve()  # This single line triggers:
```

| Mechanism | What Happens | Human Equivalent |
|-----------|--------------|------------------|
| **Decay** | Old unused memories fade (importance ‚Üí 0) | Forgetting curve |
| **Consolidation** | 5 mentions of "user likes Python" ‚Üí 1 strong memory | Sleep consolidation |
| **Rehydration** | Contradictions resolved ("CEO is John" ‚Üí "CEO is Jane") | Memory updating |
| **Importance Scoring** | PageRank recalculated | Synaptic strengthening |

**Result:** 80% memory reduction while **keeping what matters**.

### 4. üè¢ Enterprise Multi-Tenant Isolation

**Each user gets their own brain.** Complete data separation.

```python
# Alice's memory
alice = GraphMem(config, user_id="alice", memory_id="chat")
alice.ingest("I work at Google as a senior engineer")

# Bob's memory (COMPLETELY ISOLATED)
bob = GraphMem(config, user_id="bob", memory_id="chat")
bob.ingest("I'm a doctor at Mayo Clinic")

# Alice can NEVER see Bob's data
alice.query("What does Bob do?")  # ‚Üí "No information found" ‚úÖ

# Bob can NEVER see Alice's data  
bob.query("Where does Alice work?")  # ‚Üí "No information found" ‚úÖ
```

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Neo4j Global Instance                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           USER: alice              ‚îÇ            USER: bob                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ üè¢ Google  ‚Üí üë§ Alice       ‚îÇ   ‚îÇ   ‚îÇ üè• Mayo Clinic ‚Üí üë§ Bob     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     ‚Üì                       ‚îÇ   ‚îÇ   ‚îÇ       ‚Üì                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ üíº Senior Engineer          ‚îÇ   ‚îÇ   ‚îÇ   ü©∫ Doctor                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    Redis Cache (Also Isolated by user_id)                 ‚îÇ
‚îÇ  alice:query:*  alice:search:*     ‚îÇ     bob:query:*  bob:search:*       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üí° The 3-Line API

```python
from graphmem import GraphMem, MemoryConfig

# Initialize (works with ANY OpenAI-compatible API)
config = MemoryConfig(
    llm_provider="openai_compatible",
    llm_api_key="your-key",
    llm_api_base="https://openrouter.ai/api/v1",
    llm_model="google/gemini-2.0-flash-001",
    embedding_provider="openai_compatible",
    embedding_api_key="your-key",
    embedding_api_base="https://openrouter.ai/api/v1",
    embedding_model="openai/text-embedding-3-small",
)

memory = GraphMem(config)

# That's it. 3 methods:
memory.ingest("Tesla is led by CEO Elon Musk...")  # ‚Üê Extract knowledge
memory.query("Who is the CEO?")                    # ‚Üê Ask questions
memory.evolve()                                    # ‚Üê Let memory mature
```

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              üß† GraphMem                                     ‚îÇ
‚îÇ                     The Human Brain for AI Agents                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ               ‚îÇ               ‚îÇ
                    ‚ñº               ‚ñº               ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   ingest()   ‚îÇ  ‚îÇ   query()    ‚îÇ  ‚îÇ   evolve()   ‚îÇ
         ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
         ‚îÇ Learn new    ‚îÇ  ‚îÇ Recall with  ‚îÇ  ‚îÇ Mature like  ‚îÇ
         ‚îÇ information  ‚îÇ  ‚îÇ reasoning    ‚îÇ  ‚îÇ human memory ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                 ‚îÇ                 ‚îÇ
                ‚ñº                 ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         üï∏Ô∏è Knowledge Graph Engine                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  Entity         ‚îÇ  ‚îÇ  Relationship   ‚îÇ  ‚îÇ  Community      ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  Extraction     ‚îÇ  ‚îÇ  Detection      ‚îÇ  ‚îÇ  Detection      ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ LLM-based    ‚îÇ  ‚îÇ  ‚Ä¢ Temporal     ‚îÇ  ‚îÇ  ‚Ä¢ Auto-cluster ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Multi-type   ‚îÇ  ‚îÇ  ‚Ä¢ [t_s, t_e]   ‚îÇ  ‚îÇ  ‚Ä¢ Summaries    ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  Entity         ‚îÇ  ‚îÇ  PageRank       ‚îÇ  ‚îÇ  Point-in-Time  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  Resolution     ‚îÇ  ‚îÇ  Centrality     ‚îÇ  ‚îÇ  Queries        ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Canonicalize ‚îÇ  ‚îÇ  ‚Ä¢ Hub detect   ‚îÇ  ‚îÇ  ‚Ä¢ "CEO in 2015"‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Merge aliases‚îÇ  ‚îÇ  ‚Ä¢ Importance   ‚îÇ  ‚îÇ  ‚Ä¢ is_valid_at()‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                                       ‚îÇ
                ‚ñº                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    üîÑ Evolution Engine            ‚îÇ   ‚îÇ         üíæ Storage Layer            ‚îÇ
‚îÇ    (Human Memory Simulation)      ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ                                     ‚îÇ
‚îÇ                                   ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ   ‚îÇ  ‚îÇ   Neo4j     ‚îÇ  ‚îÇ   Redis     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇPageRank ‚îÇ  ‚îÇ Memory  ‚îÇ        ‚îÇ   ‚îÇ  ‚îÇ   Graph     ‚îÇ  ‚îÇ   Cache     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇCentrality‚îÇ  ‚îÇ Decay   ‚îÇ        ‚îÇ   ‚îÇ  ‚îÇ  + Temporal ‚îÇ  ‚îÇ  + Multi-   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ         ‚îÇ        ‚îÇ   ‚îÇ  ‚îÇ  + Vectors  ‚îÇ  ‚îÇ    tenant   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Hub   ‚îÇ  ‚îÇ‚Ä¢ Ebbinghaus      ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îÇ  detect ‚îÇ  ‚îÇ  curve   ‚îÇ       ‚îÇ   ‚îÇ                                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ                                   ‚îÇ   ‚îÇ  ‚îÇ   In-Memory (Default)      ‚îÇ    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ   ‚îÇ  ‚îÇ   No external DB needed    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇConsolid-‚îÇ  ‚îÇTemporal ‚îÇ        ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ  ‚îÇation    ‚îÇ  ‚îÇValidity ‚îÇ        ‚îÇ   ‚îÇ                                     ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Merge ‚îÇ  ‚îÇ‚Ä¢ Supersede       ‚îÇ   ‚îÇ                                     ‚îÇ
‚îÇ  ‚îÇ  similar‚îÇ  ‚îÇ‚Ä¢ History ‚îÇ       ‚îÇ   ‚îÇ                                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ   ‚îÇ                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Why Enterprise Teams Choose GraphMem

### Production Scale Performance

| Metric | Naive RAG | GraphMem | Advantage |
|--------|-----------|----------|-----------|
| **1K conversations** | üí• Context overflow | ‚úÖ Bounded | Handles growth |
| **10K entities** | O(n) = 2.3s | O(1) = 50ms | **46x faster** |
| **1 year history** | 3,650 entries | ~100 consolidated | **97% reduction** |
| **Entity conflicts** | Duplicates | Auto-resolved | Clean data |
| **Temporal queries** | ‚ùå Impossible | ‚úÖ Native | Unique capability |

### Cost Efficiency

```
Naive RAG:  Send entire history every query    = $$$$$
GraphMem:   Retrieve only relevant subgraph    = $
                                                 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                                                 99% savings
```

### Enterprise Requirements

| Requirement | GraphMem |
|-------------|----------|
| Multi-tenant isolation | ‚úÖ `user_id` on every node |
| ACID transactions | ‚úÖ Neo4j backend |
| Horizontal scaling | ‚úÖ Neo4j cluster + Redis |
| Audit trail | ‚úÖ Temporal validity history |
| Data sovereignty | ‚úÖ Self-hosted option |

---

## üîß Installation

```bash
# Core (in-memory, no dependencies)
pip install agentic-graph-mem

# Production (Neo4j + Redis)
pip install "agentic-graph-mem[all]"
```

---

## üìñ Complete Examples

### Basic Usage

```python
from graphmem import GraphMem, MemoryConfig

config = MemoryConfig(
    llm_provider="openai_compatible",
    llm_api_key="sk-or-v1-your-key",
    llm_api_base="https://openrouter.ai/api/v1",
    llm_model="google/gemini-2.0-flash-001",
    embedding_provider="openai_compatible",
    embedding_api_key="sk-or-v1-your-key",
    embedding_api_base="https://openrouter.ai/api/v1",
    embedding_model="openai/text-embedding-3-small",
)

memory = GraphMem(config)

# Learn
memory.ingest("Tesla is led by CEO Elon Musk. Founded in 2003.")
memory.ingest("SpaceX, founded by Elon Musk in 2002, builds rockets.")
memory.ingest("Neuralink develops brain-computer interfaces.")

# Recall
response = memory.query("What companies does Elon Musk lead?")
print(response.answer)  # "Elon Musk leads Tesla, SpaceX, and Neuralink."

# Mature
memory.evolve()  # Consolidates, decays, re-ranks importance
```

### Production: Multi-Tenant Chat System

```python
from graphmem import GraphMem, MemoryConfig

# Base config (shared across all users)
base_config = MemoryConfig(
    llm_provider="openai_compatible",
    llm_api_key="sk-or-v1-your-key",
    llm_api_base="https://openrouter.ai/api/v1",
    llm_model="google/gemini-2.0-flash-001",
    embedding_provider="openai_compatible",
    embedding_api_key="sk-or-v1-your-key",
    embedding_api_base="https://openrouter.ai/api/v1",
    embedding_model="openai/text-embedding-3-small",
    # Production storage
    neo4j_uri="neo4j+ssc://xxx.databases.neo4j.io",
    neo4j_username="neo4j",
    neo4j_password="your-password",
    redis_url="redis://default:password@your-redis.cloud.redislabs.com:17983",
)

class ChatService:
    def get_memory(self, user_id: str, session_id: str) -> GraphMem:
        """Each user gets isolated memory."""
        return GraphMem(
            base_config,
            user_id=user_id,      # ‚Üê Complete isolation
            memory_id=session_id,  # ‚Üê Per-session memory
        )
    
    def chat(self, user_id: str, session_id: str, message: str) -> str:
        memory = self.get_memory(user_id, session_id)
        
        # Store user message as memory
        memory.ingest(message)
        
        # Generate response using memory
        response = memory.query(message)
        
        return response.answer

# Usage
service = ChatService()

# Alice's session (isolated)
alice_response = service.chat("alice", "session_1", "I'm a software engineer at Google")
alice_response = service.chat("alice", "session_1", "What do I do?")  # ‚Üí "Software engineer at Google"

# Bob's session (completely separate)
bob_response = service.chat("bob", "session_1", "I'm a doctor")
bob_response = service.chat("bob", "session_1", "What does Alice do?")  # ‚Üí "No information found"
```

### Temporal Queries: Track Changes Over Time

```python
from datetime import datetime
from graphmem.core.memory_types import MemoryEdge
from graphmem.stores.neo4j_store import Neo4jStore

store = Neo4jStore(uri, user, password)

# Track CEO transitions
john_ceo = MemoryEdge(
    id="john_ceo",
    source_id="john_smith",
    target_id="acme_corp",
    relation_type="CEO_OF",
    valid_from=datetime(2010, 1, 1),
    valid_until=datetime(2018, 6, 30),  # John left
)

jane_ceo = MemoryEdge(
    id="jane_ceo",
    source_id="jane_doe",
    target_id="acme_corp",
    relation_type="CEO_OF",
    valid_from=datetime(2018, 7, 1),
    valid_until=None,  # Current CEO
)

# Query by time period
ceo_2015 = store.query_edges_at_time(
    memory_id="company_kb",
    query_time=datetime(2015, 6, 1),
    relation_type="CEO_OF"
)
# ‚Üí Returns John Smith's edge

ceo_now = store.query_edges_at_time(
    memory_id="company_kb",
    query_time=datetime.utcnow(),
    relation_type="CEO_OF"
)
# ‚Üí Returns Jane Doe's edge

# Mark relationship as ended
store.supersede_relationship(
    memory_id="company_kb",
    edge_id="jane_ceo",
    end_time=datetime(2025, 12, 31)  # Jane leaves
)
```

---

## üß™ Run the Evaluation

```bash
cd graphmem/evaluation
python run_eval.py
```

Uses [MultiHopRAG dataset](https://huggingface.co/datasets/yixuantt/MultiHopRAG) (2,556 QA samples, 609 documents).

---

## üî¨ The Science Behind GraphMem

### Ebbinghaus Forgetting Curve
```
importance(t) = importance_0 √ó e^(-Œª √ó (t - last_access))
```
Just like human memory, unused information fades exponentially.

### PageRank for Entity Importance
```
PR(A) = (1-d) + d √ó Œ£(PR(Ti)/C(Ti))
```
Hub entities (connected to many concepts) are more important‚Äîexactly like neural hubs in the brain.

### Temporal Validity
```
valid(r, t) = 1[t_s(r) ‚â§ t ‚â§ t_e(r)]
```
Every relationship has a time interval, enabling episodic memory recall.

---

## üè≠ Deployment Tiers

| Scale | Users | Strategy | Infrastructure |
|-------|-------|----------|----------------|
| **Startup** | 1-100 | Single Neo4j, user_id filtering | Neo4j Aura Free |
| **Growth** | 100-10K | Single Neo4j + Redis | Neo4j Aura Pro + Redis Cloud |
| **Enterprise** | 10K-100K | Sharded by region | Neo4j Enterprise Cluster |
| **Global** | 100K+ | Database per tenant | Multi-region Neo4j Fabric |

---

## üì¶ Dependencies

```bash
# Core (no external services)
pip install agentic-graph-mem

# With Neo4j persistence
pip install "agentic-graph-mem[neo4j]"

# With Redis caching
pip install "agentic-graph-mem[redis]"

# Full production stack
pip install "agentic-graph-mem[all]"
```

---

## üéØ The Future of AI Memory

GraphMem isn't just another vector database wrapper. It's a **paradigm shift**:

| Old Way | GraphMem Way |
|---------|--------------|
| Store everything | Remember what matters |
| Static forever | Evolves over time |
| No relationships | Rich knowledge graph |
| "Who is CEO?" | "Who was CEO in 2015?" |
| One user fits all | Enterprise multi-tenant |
| Hope for the best | PageRank prioritization |

**The agents of tomorrow will have memories that think.**

---

## ü§ù Contributing

We're building the future of AI memory. Join us!

- üêõ [Report bugs](https://github.com/Al-aminI/GraphMem/issues)
- üí° [Request features](https://github.com/Al-aminI/GraphMem/issues)
- üîÄ [Submit PRs](https://github.com/Al-aminI/GraphMem/pulls)

---

## üìÑ License

MIT License - see [LICENSE](LICENSE).

---

## üôè Acknowledgments

- Inspired by cognitive neuroscience research on human memory
- Built on Neo4j, Redis, and OpenAI
- PageRank algorithm by Larry Page and Sergey Brin

---

<div align="center">

**Made with üß† by Al-Amin Ibrahim**

[![GitHub](https://img.shields.io/badge/GitHub-Al--aminI/GraphMem-blue?style=for-the-badge&logo=github)](https://github.com/Al-aminI/GraphMem)
[![PyPI](https://img.shields.io/badge/PyPI-agentic--graph--mem-green?style=for-the-badge&logo=pypi)](https://pypi.org/project/agentic-graph-mem/)

*"Give your AI agents the memory they deserve."*

</div>
